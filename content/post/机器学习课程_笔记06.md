---
title: 机器学习课程_笔记06
date: 2017-06-16 19:00:00+08:00
author: 徐新杰
tags:
  - 机器学习
  - andrew ng
categories:
  - 机器学习
---

## 多项式事件模型

面的这种基本的朴素贝叶斯模型叫做多元伯努利事件模型，该模型有多种扩展，一种是每个分量的多值化，即将$P(X_i|y)$由伯努利分布扩展到多项式分布；还有一种是将连续变量值离散化。例如以房屋面积为例：

| Living area(sq. feet) | <400 | 400-800 | 800-1200 | 1200-1600 | >1600 |
| :-------------------: | :--: | :-----: | :------: | :-------: | :---: |
|         $X_i$         |  1   |    2    |    3     |     4     |   5   |

还有一种，与多元伯努利有较大区别的朴素贝叶斯模型，就是**多项式事件模型**。

多项式事件模型改变了特征向量的表示方法：

在多元伯努利模型中，特征向量的每个分量代表词典中该index上的词语是否在文本中出现过，其取值范围为{0,1}，特征向量的长度为词典的大小。

而在多项式事件模型中，特征向量中的每个分量的值是文本中处于该分量位置的单词在词典中的索引，其取值范围是{1,2,...,|V|}，|V|是词典的大小，特征向量的长度为文本中单词的数量。

例如：在多元伯努利模型下，一篇文本的特征向量可能如下：

​                                                                           ![img](http://img.blog.csdn.net/20160814160719280)

在多项式事件模型下，这篇文本的特征向量为：

​                                                                       ![img](http://img.blog.csdn.net/20160814160724858)

一篇文本产生的过程是：

1、确定文本类别

2、以相同的多项式分布在各个位置上生成词语。

例如：x1是由服从p(x1|y)的多项式分布产生的，x2是独立与x1的并且来自于同一个多项式分布，同样的，产生x3,x4，一直到xn。

因此，所有的这个信息的概率是`$\Pi_{i=1}^nP(X_i|y)$`

模型的参数为： 
`$$
\phi_y = P(y) \\
\phi_{i|y=1} = P(X_j=i|y=1) \\
\phi_{i|y=0} = P(X_j=i|y=0) 
$$`
参数在训练集上的极大似然函数： 

​                                                       ![这里写图片描述](http://img.blog.csdn.net/20150715150944040)

参数的最大似然估计为： 
​                                                       ![这里写图片描述](http://img.blog.csdn.net/20150715151023799)

应用laplace平滑，分子加1，分母加|V|，得到： 
​                                                     ![这里写图片描述](http://img.blog.csdn.net/20150715151210729)

对于式子：

​                                                         ![img](http://img.blog.csdn.net/20160814162413600)

分子的意思是对训练集合中的所有垃圾邮件中词k出现的次数进行求和。

分母的含义是对训练样本集合进行求和，如果其中的一个样本是垃圾邮件（y=1），那么就把它的长度加起来，所以分母的含义是训练集合中所有垃圾邮件的词语总长。

所以这个比值的含义就是在所有垃圾邮件中，词k所占的比例。
注意这个公式与多元伯努利的不同在于：这里针对整体样本求的φk|y=1 ，而多远伯努利里面针对每个特征求的φxj=1|y=1 ，而且这里的特征值维度不一定是相同的。



### 举例说明多项式事件模型

假设邮件中有a,b,c三个词，他们在词典的位置分别是1,2,3,第一封里面内容为a,b，第二封为b,a;第三封为a,c,b,第四封为c,c,c,a。Y=1是垃圾邮件。
因此，我们有：
![img](http://img.blog.csdn.net/20160814163548178)
那么，我们可得：
![img](http://img.blog.csdn.net/20160814163605682)
假如有一封信的邮件，内容为b,c。那么它的特征向量为{2,3},我们可得：
![img](http://img.blog.csdn.net/20160814164019527)

那么该邮件为垃圾邮件概率是0.6。

## 先验分布、后验分布、似然估计的概念

用“瓜熟蒂落”这个因果例子，从概率（probability）的角度说一下，

**先验概率**，就是常识、经验所透露出的“因”的概率，即瓜熟的概率。应该很清楚。

**后验概率**，就是在知道“果”之后，去推测“因”的概率，也就是说，如果已经知道瓜蒂脱落，那么瓜熟的概率是多少。后验和先验的关系可以通过贝叶斯公式来求。也就是：

P（瓜熟 | 已知蒂落）=P（瓜熟）×P（蒂落 | 瓜熟）/ P（蒂落）

**似然函数**，是根据已知结果去推测固有性质的可能性（likelihood），是对固有性质的拟合程度，所以不能称为概率。在这里就是说，不要管什么瓜熟的概率，只care瓜熟与蒂落的关系。如果蒂落了，那么对瓜熟这一属性的拟合程度有多大。似然函数，一般写成L（瓜熟 | 已知蒂落），和后验概率非常像，区别在于似然函数把瓜熟看成一个肯定存在的属性，而后验概率把瓜熟看成一个随机变量。



再扯一扯似然函数和**条件概率**的关系。似然函数就是条件概率的逆反。意为：

L（瓜熟 | 已知蒂落）= C × P（蒂落 | 瓜熟），C是常数。具体来说，现在有1000个瓜熟了，落了800个，那条件概率是0.8。那我也可以说，这1000个瓜都熟的可能性是0.8C。

注意，之所以加个常数项，是因为似然函数的具体值没有意义，只有看它的相对大小或者两个似然值的比率才有意义，后面还有例子。



同理，如果理解上面的意义，分布就是一“串”概率。

**先验分布**：现在常识不但告诉我们瓜熟的概率，也说明了瓜青、瓜烂的概率

**后验分布**：在知道蒂落之后，瓜青、瓜熟、瓜烂的概率都是多少

**似然函数**：在知道蒂落的情形下，如果以瓜青为必然属性，它的可能性是多少？如果以瓜熟为必然属性，它的可能性是多少？如果以瓜烂为必然属性，它的可能性是多少？似然函数不是分布，只是对上述三种情形下各自的可能性描述。

那么我们把这三者结合起来，就可以得到：后验分布 正比于 先验分布 × 似然函数。先验就是设定一种情形，似然就是看这种情形下发生的可能性，两者合起来就是后验的概率。

至于**似然估计**：

就是不管先验和后验那一套，只看似然函数，现在蒂落了，可能有瓜青、瓜熟、瓜烂，这三种情况都有个似然值（L（瓜青）：0.6、L（瓜熟）：0.8、L（瓜烂）：0.7），我们采用最大的那个，即瓜熟，这个时候假定瓜熟为必然属性是最有可能的。

## 神经网络

对于之前学习的分类算法，我们的目标都是求解一条直线，这条直线将数据进行分类，但如果数据并不是线性可分的话，这些模型的性能会变差。针对非线性分类的问题，出现了很多分类算法，神经网络是其中最早出现的一种。

例如，下图使用Logistic模型分类，得到的是图中的直线，但这条直线并不是很合理，我们希望得到图中的曲线：

![img](http://img.blog.csdn.net/20160815135742557)

假设特征向量为{x0,x1,x2,x3}sigmoid代表计算节点,output是函数输出对于Logistic模型来说，过程如图：

![img](http://img.blog.csdn.net/20160815135806713)

Sigmoid计算节点含有参数θ，其函数形式为：

![img](http://img.blog.csdn.net/20160815135828370)

但对于神经网络来说，过程如图：

![img](http://img.blog.csdn.net/20160815135910621)

特征向量输入到多个sigmoid单元，然后这些单元再输入到一个sigmoid单元，这些中间节点叫做隐藏层，神经网络可以有多个隐藏层.。

其中的参数分别为：

​                                          ![img](http://img.blog.csdn.net/20160815135956906)

求解其中的参数，需要使用成本函数：

​                                                   ![img](http://img.blog.csdn.net/20160815140020594)

然后通过梯度下降方法求得参数值，在神经网络模型中，梯度下降算法有一个专有的名字叫做：反向传播算法。

神经网络算法的特点：

* 不知道隐藏层计算的东西的意义。
* 有很多的局部最优解，需要通过多次随机设定初始值然后运行梯度下降算法获得全局最优值。



## 支持向量机(SVM)

在了解支持向量机之前，我们需要知道**函数间隔**和**几何间隔**。

首先，我们先定义新的标记：

1、用g(z)∈{-1,1}代替y(x)∈{0,1}。
2、目标函数从![img](http://img.blog.csdn.net/20160815140241674)变为![img](http://img.blog.csdn.net/20160815140246985)。

​       （这里b代替了![这里写图片描述](http://img.blog.csdn.net/20150715163836120)的角色，w代替![这里写图片描述](http://img.blog.csdn.net/20150715163939522)         的角色，ω和b可以确定唯一的一个超平面）

点(x(i),y(i))到由ω，b决定的超平面的**函数间隔**是：

**                                                            **![这里写图片描述](http://img.blog.csdn.net/20150715164202098) 

从上面的十字可以看出：如果![这里写图片描述](http://img.blog.csdn.net/20150715164506049)，为了使函数间隔很大，![这里写图片描述](http://img.blog.csdn.net/20150715164628688)需要是一个很大的正数。如果![这里写图片描述](http://img.blog.csdn.net/20150715164800124)为了使函数间隔很大，需要![这里写图片描述](http://img.blog.csdn.net/20150715164905941)是一个很大的负数.。如果![这里写图片描述](http://img.blog.csdn.net/20150715165020345)，则我们的预测结果是正确的。**因此，函数间隔越大，说明预测结果越是确定正确的。**

如果我们用2w代替w，用2b代替b，那么由于![这里写图片描述](http://img.blog.csdn.net/20150716095256851)，不会对![这里写图片描述](http://img.blog.csdn.net/20150716095343603)有任何改变，也就是说![这里写图片描述](http://img.blog.csdn.net/20150716095431696)只是取决于符号而跟数量没有关系.。但是用（2w,2b）代替（w,b）会使得函数间隔间隔增大两倍。

超平面与整个训练集合的函数间隔是：

​                                                                        ![这里写图片描述](http://img.blog.csdn.net/20150716100248666)

**为了解决这**函数间隔无意义增大的问题**，就有了几何间隔的定义，几何间隔定义如下：**

​                                                              ![这里写图片描述](http://img.blog.csdn.net/20150716110006264)

用下图说明几何间隔的问题：

​                                                            ![这里写图片描述](http://img.blog.csdn.net/20150716110046968)

上图中，w垂直于分隔超平面，训练样本A，它到分隔线 的距离是![这里写图片描述](http://img.blog.csdn.net/20150716110321607)，也就是线段AB的长度. ![这里写图片描述](http://img.blog.csdn.net/20150716110423965)是单位向量（unit-length vector）, B点表示为为：![这里写图片描述](http://img.blog.csdn.net/20150716111726364)，在分隔线上的所有点满足![这里写图片描述](http://img.blog.csdn.net/20150716111930453)因此有： 

​                                                                            ![这里写图片描述](http://img.blog.csdn.net/20150716112000704) 
解到： 
​                                                                       ![这里写图片描述](http://img.blog.csdn.net/20150716112030453)

由上式可知：当||w||等于1，几何间隔等于函数间隔.    **但几何间隔是不会随着参数的调整而变化的。**

超平面与整个训练集合的几何间隔是：
​                                                                                  ![这里写图片描述](http://img.blog.csdn.net/20150716112417673)

有了几何间隔和函数间隔，使得我们的分类结果不仅能保证正确性，还可以保证分类结果的确定性。
